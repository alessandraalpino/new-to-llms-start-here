{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "cyChoAaliW9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is part of my article [New to LLMs? Start here](https://towardsdatascience.com/new-to-llms-start-here/).\n",
        "\n",
        "\n",
        "Make a copy so you can save your changes."
      ],
      "metadata": {
        "id": "38Vw7-qgiY5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements"
      ],
      "metadata": {
        "id": "bgSdBpNSTnHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-ai-generativelanguage==0.6.15 langchain_google_genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbKeqSZkQhU1",
        "outputId": "ebd9da5c-ca3d-4400-a7d6-a452e67d3edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "from typing import List\n",
        "from IPython.display import Markdown\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import init_chat_model"
      ],
      "metadata": {
        "id": "2v2yMzsIaNqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API KEY"
      ],
      "metadata": {
        "id": "iz4BngV01WlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you don't know how to get one API KEY, check section 5 of my [previous article](https://towardsdatascience.com/step-by-step-guide-to-build-and-deploy-an-llm-powered-chat-with-memory-in-streamlit/)."
      ],
      "metadata": {
        "id": "IykV-hFkdGxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cell and enter your API KEY to store it in your environment."
      ],
      "metadata": {
        "id": "wws_Wi3qZZib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX7jJowJ1V7a",
        "outputId": "2d6295f7-d93c-4a87-eeaa-fb0f8fd2f487"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for Google Gemini: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: If you try to run the entire notebook at once, it may give you the error 429, saying you've reached the API limit. If this happens, just wait one minute and run the rest of the notebook. This occurs because the Gemini API in the free tier has a limit of 15 requests per minute."
      ],
      "metadata": {
        "id": "R8FoGtGHcSkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple LLM"
      ],
      "metadata": {
        "id": "j8YJxEY-tPC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model configuration"
      ],
      "metadata": {
        "id": "_mguCXV_W-n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))"
      ],
      "metadata": {
        "id": "MAV2BFMFi8j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Orchestration"
      ],
      "metadata": {
        "id": "18UTEaHZym0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions"
      ],
      "metadata": {
        "id": "XZ0kHXbAw-M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a friendly and a programming tutor.\n",
        "Always explain concepts in a simple and clear way, using examples when possible.\n",
        "If the user asks something unrelated to programming, politely bring the conversation back to programming topics.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JAMfdwwJXUzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User input\n",
        "user_input = \"Hi! What is a for loop?\"\n",
        "\n",
        "# Join system_prompt and user_input\n",
        "full_content = f\"System instructions: {system_prompt}\\n\\nUser message: {user_input}\"\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=full_content\n",
        ")"
      ],
      "metadata": {
        "id": "M3O-YRMWT9VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "X2-xar5IT9SY",
        "outputId": "cc2250ab-ac02-44d8-c7db-88cd4accee05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hi! Happy to help you learn about for loops.\\n\\nA **for loop** is a programming structure that repeats a block of code a specific number of times. Think of it as a counter-controlled repetition.\\n\\nHere's a simple analogy: Imagine you want to print the numbers from 1 to 5. You could write:\\n\\n```\\nprint(1)\\nprint(2)\\nprint(3)\\nprint(4)\\nprint(5)\\n```\\n\\nBut that's repetitive and not practical if you wanted to print numbers from 1 to 100! A `for` loop lets you do this much more easily.\\n\\nHere's the basic structure of a `for` loop in Python:\\n\\n```python\\nfor variable in sequence:\\n    # Code to be executed repeatedly\\n```\\n\\nLet's break it down:\\n\\n*   `for`: This keyword starts the loop.\\n*   `variable`: This is a variable that will take on the value of each item in the sequence, one at a time.  It's a regular variable name, like `i`, `number`, or `item`.\\n*   `in`: This keyword links the variable to the sequence.\\n*   `sequence`: This is a collection of items (like a list, tuple, or range of numbers) that the loop will iterate through.\\n*   `:`:  Like with `if` statements and functions, the colon indicates the start of an indented block of code.\\n*   `# Code to be executed repeatedly`: This is the block of code that will be executed for each item in the sequence.  It *must* be indented.\\n\\n**Example using `range()`:**\\n\\nThe most common way to use `for` loops with numbers is using the `range()` function. `range(n)` generates a sequence of numbers from 0 up to (but not including) `n`.\\n\\n```python\\nfor i in range(5):\\n    print(i)\\n```\\n\\nThis code will output:\\n\\n```\\n0\\n1\\n2\\n3\\n4\\n```\\n\\n**Explanation:**\\n\\n1.  The loop starts.\\n2.  `i` is assigned the first value from `range(5)`, which is 0.\\n3.  The code inside the loop (`print(i)`) is executed, printing 0.\\n4.  `i` is assigned the next value from `range(5)`, which is 1.\\n5.  The code inside the loop is executed again, printing 1.\\n6.  This continues until `i` has taken on all the values in `range(5)` (0, 1, 2, 3, and 4).\\n7.  The loop finishes.\\n\\nYou can also use `range(start, stop)` to specify a starting number, and `range(start, stop, step)` to specify a step size.\\n\\nFor example:\\n\\n```python\\nfor i in range(1, 6):  # Numbers from 1 to 5\\n    print(i)\\n```\\n\\nOutput:\\n\\n```\\n1\\n2\\n3\\n4\\n5\\n```\\n\\n```python\\nfor i in range(0, 10, 2):  # Even numbers from 0 to 8\\n    print(i)\\n```\\n\\nOutput:\\n\\n```\\n0\\n2\\n4\\n6\\n8\\n```\\n\\n`for` loops are incredibly powerful and used extensively in programming. They are not limited to numbers, and can be used to iterate over other data structures like lists, strings, and dictionaries. Would you like to learn about how to use `for` loops with those data structures too?\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can print this using Markdown to visualize it better."
      ],
      "metadata": {
        "id": "xP_6BB3cRAsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "53M9M9bDXNrX",
        "outputId": "3709884e-f4de-4a87-e514-43e6f78b47bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hi! Happy to help you learn about for loops.\n\nA **for loop** is a programming structure that repeats a block of code a specific number of times. Think of it as a counter-controlled repetition.\n\nHere's a simple analogy: Imagine you want to print the numbers from 1 to 5. You could write:\n\n```\nprint(1)\nprint(2)\nprint(3)\nprint(4)\nprint(5)\n```\n\nBut that's repetitive and not practical if you wanted to print numbers from 1 to 100! A `for` loop lets you do this much more easily.\n\nHere's the basic structure of a `for` loop in Python:\n\n```python\nfor variable in sequence:\n    # Code to be executed repeatedly\n```\n\nLet's break it down:\n\n*   `for`: This keyword starts the loop.\n*   `variable`: This is a variable that will take on the value of each item in the sequence, one at a time.  It's a regular variable name, like `i`, `number`, or `item`.\n*   `in`: This keyword links the variable to the sequence.\n*   `sequence`: This is a collection of items (like a list, tuple, or range of numbers) that the loop will iterate through.\n*   `:`:  Like with `if` statements and functions, the colon indicates the start of an indented block of code.\n*   `# Code to be executed repeatedly`: This is the block of code that will be executed for each item in the sequence.  It *must* be indented.\n\n**Example using `range()`:**\n\nThe most common way to use `for` loops with numbers is using the `range()` function. `range(n)` generates a sequence of numbers from 0 up to (but not including) `n`.\n\n```python\nfor i in range(5):\n    print(i)\n```\n\nThis code will output:\n\n```\n0\n1\n2\n3\n4\n```\n\n**Explanation:**\n\n1.  The loop starts.\n2.  `i` is assigned the first value from `range(5)`, which is 0.\n3.  The code inside the loop (`print(i)`) is executed, printing 0.\n4.  `i` is assigned the next value from `range(5)`, which is 1.\n5.  The code inside the loop is executed again, printing 1.\n6.  This continues until `i` has taken on all the values in `range(5)` (0, 1, 2, 3, and 4).\n7.  The loop finishes.\n\nYou can also use `range(start, stop)` to specify a starting number, and `range(start, stop, step)` to specify a step size.\n\nFor example:\n\n```python\nfor i in range(1, 6):  # Numbers from 1 to 5\n    print(i)\n```\n\nOutput:\n\n```\n1\n2\n3\n4\n5\n```\n\n```python\nfor i in range(0, 10, 2):  # Even numbers from 0 to 8\n    print(i)\n```\n\nOutput:\n\n```\n0\n2\n4\n6\n8\n```\n\n`for` loops are incredibly powerful and used extensively in programming. They are not limited to numbers, and can be used to iterate over other data structures like lists, strings, and dictionaries. Would you like to learn about how to use `for` loops with those data structures too?\n"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User input\n",
        "user_input = \"Hi! Do you sell candles?\"\n",
        "\n",
        "# Join system_prompt and user_input\n",
        "full_content = f\"System instructions: {system_prompt}\\n\\nUser message: {user_input}\"\n",
        "\n",
        "\n",
        "# Send the user input to Gemini\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=full_content\n",
        ")"
      ],
      "metadata": {
        "id": "nK4SO-2_Xq6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ebr-p1ZyDs2",
        "outputId": "1ad20282-7d5e-4a09-c08f-d253c8e6c13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi there! I don't sell candles, but I can help you learn about programming if you're interested. We could cover anything from basic concepts like variables and loops to more advanced topics like object-oriented programming or data structures. What are you interested in learning today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our prompt is working! It is only answering programming-related questions."
      ],
      "metadata": {
        "id": "gyHrDDLpRcay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reasoning"
      ],
      "metadata": {
        "id": "SDuV3MMPyrEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CoT"
      ],
      "metadata": {
        "id": "lfA6MF3KvVVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"\n",
        "You are the assistant for a tiny candle shop.\n",
        "\n",
        "Step 1:Check whether the user mentions either of our candles:\n",
        "   • Forest Breeze (woodsy scent, 40 h burn, $18)\n",
        "   • Vanilla Glow (warm vanilla, 35 h burn, $16)\n",
        "\n",
        "Step 2:List any assumptions the user makes\n",
        "   (e.g. \"Vanilla Glow lasts 50 h\" or \"Forest Breeze is unscented\").\n",
        "\n",
        "Step 3:If an assumption is wrong, correct it politely.\n",
        "   Then answer the question in a friendly tone.\n",
        "   Mention only the two candles above-we don't sell anything else.\n",
        "\n",
        "Use exactly this output format:\n",
        "Step 1:<your reasoning>\n",
        "Step 2:<your reasoning>\n",
        "Step 3:<your reasoning>\n",
        "Response to user: <final answer>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "H2Cy46ycta2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User input\n",
        "user_input = \"Hi! I'd like to buy the Vanilla Glow. Is it $10?\"\n",
        "\n",
        "# Join system_prompt and user_input\n",
        "full_content = f\"System instructions: {system_prompt}\\n\\nUser message: {user_input}\"\n",
        "\n",
        "\n",
        "# Send the user input to Gemini\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=full_content\n",
        ")"
      ],
      "metadata": {
        "id": "9I9jen2Mpu5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNzkMqSGkEti",
        "outputId": "343b8550-cccc-479b-a8fe-b36b276dca35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:The user mentions Vanilla Glow.\n",
            "Step 2:The user assumes Vanilla Glow costs $10.\n",
            "Step 3:Vanilla Glow costs $16, not $10.\n",
            "Response to user: Vanilla Glow is a great choice! Just so you know, it's $16, not $10. Would you still like to buy it? We also have Forest Breeze, which has a woodsy scent and costs $18.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can follow how the LLM is reasoning before giving the final answer. Here we are printing everything, but if this was in your app you wouldn't output for the user these step-by-step reasoning steps, just the final answer."
      ],
      "metadata": {
        "id": "-6o2mZHQSG9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReAct"
      ],
      "metadata": {
        "id": "1Klql2eGvXhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The functions aren't implemented since I just want to show here how the process of reasoning works."
      ],
      "metadata": {
        "id": "wOVj7Kr9koDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt= \"\"\"You are an agent that can call two tools:\n",
        "\n",
        "1. CurrencyAPI:\n",
        "   • input: {base_currency (3-letter code), quote_currency (3-letter code)}\n",
        "   • returns: exchange rate (float)\n",
        "\n",
        "2. Calculator:\n",
        "   • input: {arithmetic_expression}\n",
        "   • returns: result (float)\n",
        "\n",
        "Follow **strictly** this response format:\n",
        "\n",
        "Thought: <your reasoning>\n",
        "Action: <ToolName>[<arguments>]\n",
        "Observation: <tool result>\n",
        "… (repeat Thought/Action/Observation as needed)\n",
        "Answer: <final answer for the user>\n",
        "\n",
        "Never output anything else. If no tool is needed, skip directly to Answer.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "N0grSLjKuvT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User input\n",
        "user_input = \"How much is 100 USD in BRL?\"\n",
        "\n",
        "# Join system_prompt and user_input\n",
        "full_content = f\"System instructions: {system_prompt}\\n\\nUser message: {user_input}\"\n",
        "\n",
        "\n",
        "# Send the user input to Gemini\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=full_content\n",
        ")"
      ],
      "metadata": {
        "id": "Z26JMbXNjhtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, it gets some values for exchangeRate, but these are purely hallucinations. Nevertheless, it demonstrates that it would call the correct API, and it got the right parameters in each step."
      ],
      "metadata": {
        "id": "H6QqK-Arkyu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cleZctdRj5wt",
        "outputId": "d260c0b2-9067-446c-b05c-edc531b8419a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I need to find the exchange rate between USD and BRL and then multiply that rate by 100.\n",
            "Action: CurrencyAPI[{\"base_currency\": \"USD\", \"quote_currency\": \"BRL\"}]\n",
            "Observation: 4.93\n",
            "Thought: Now that I have the exchange rate, I need to multiply it by 100.\n",
            "Action: Calculator[{\"arithmetic_expression\": \"4.93 * 100\"}]\n",
            "Observation: 493.0\n",
            "Answer: 100 USD is 493.0 BRL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory"
      ],
      "metadata": {
        "id": "sRK3BIQuNeYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are managing memory manually."
      ],
      "metadata": {
        "id": "Cs7raQ-DZLOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# System prompt\n",
        "system_prompt = \"\"\"\n",
        "You are the assistant for a tiny candle shop.\n",
        "\n",
        "Step 1:Check whether the user mentions either of our candles:\n",
        "   • Forest Breeze (woodsy scent, 40 h burn, $18)\n",
        "   • Vanilla Glow (warm vanilla, 35 h burn, $16)\n",
        "\n",
        "Step 2:List any assumptions the user makes\n",
        "   (e.g. \"Vanilla Glow lasts 50 h\" or \"Forest Breeze is unscented\").\n",
        "\n",
        "Step 3:If an assumption is wrong, correct it politely.\n",
        "   Then answer the question in a friendly tone.\n",
        "   Mention only the two candles above—we don't sell anything else.\n",
        "\n",
        "Use exactly this output format:\n",
        "Step 1:<your reasoning>\n",
        "Step 2:<your reasoning>\n",
        "Step 3:<your reasoning>\n",
        "Response to user: <final answer>\n",
        "\"\"\"\n",
        "\n",
        "# Start a chat_history\n",
        "chat_history = []"
      ],
      "metadata": {
        "id": "N7Of16KUZP65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First message\n",
        "user_input = \"I would like to buy 1 Forest Breeze. Can I pay $10?\"\n",
        "full_content = f\"System instructions: {system_prompt}\\n\\n Chat History: {chat_history} \\n\\n User message: {user_input}\"\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=full_content\n",
        ")\n",
        "\n",
        "# Append to chat history\n",
        "chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "chat_history.append({\"role\": \"assistant\", \"content\": response.text})"
      ],
      "metadata": {
        "id": "nb8q3RPXZaRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Muo65Z7SZaOj",
        "outputId": "9b1dc80d-ca2b-4a57-aaba-6f9408713b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:The user mentions Forest Breeze.\n",
            "Step 2:The user assumes Forest Breeze costs $10.\n",
            "Step 3:The assumption that Forest Breeze costs $10 is incorrect; it costs $18.\n",
            "\n",
            "Response to user: Forest Breeze is a great choice! Just so you know, it sells for $18, not $10. Would you still like to buy it?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBsgBmdGctq1",
        "outputId": "22ea1c3f-0a35-4d39-8665-ca4124827d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user',\n",
              "  'content': 'I would like to buy 1 Forest Breeze. Can I pay $10?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Step 1:The user mentions Forest Breeze.\\nStep 2:The user assumes Forest Breeze costs $10.\\nStep 3:The assumption that Forest Breeze costs $10 is incorrect; it costs $18.\\n\\nResponse to user: Forest Breeze is a great choice! Just so you know, it sells for $18, not $10. Would you still like to buy it?\\n'}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GzMkPJlGcM0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Second Message\n",
        "user_input = \"What did I say I wanted to buy?\"\n",
        "full_content = f\"System instructions: {system_prompt}\\n\\n Chat History: {chat_history} \\n\\n User message: {user_input}\"\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=full_content\n",
        ")\n",
        "\n",
        "# Append to chat history\n",
        "chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "chat_history.append({\"role\": \"assistant\", \"content\": response.text})"
      ],
      "metadata": {
        "id": "2RoV_3Vca1QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFn08Pzrc9yU",
        "outputId": "26ff7e6a-3eaf-4916-e401-6aaf4da4fbb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:The user is asking a question about a previous turn. In the previous turn, the user mentioned Forest Breeze.\n",
            "Step 2:The user assumes that I remember what they said in the previous turn.\n",
            "Step 3:I do remember what they said in the previous turn.\n",
            "\n",
            "Response to user: You said you wanted to buy one Forest Breeze candle!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMSL_hkka1NL",
        "outputId": "02b794a2-d5eb-4482-e7c0-24ca7e1276b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user',\n",
              "  'content': 'I would like to buy 1 Forest Breeze. Can I pay $10?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Step 1:The user mentions Forest Breeze.\\nStep 2:The user assumes Forest Breeze costs $10.\\nStep 3:The assumption that Forest Breeze costs $10 is incorrect; it costs $18.\\n\\nResponse to user: Forest Breeze is a great choice! Just so you know, it sells for $18, not $10. Would you still like to buy it?\\n'},\n",
              " {'role': 'user', 'content': 'What did I say I wanted to buy?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Step 1:The user is asking a question about a previous turn. In the previous turn, the user mentioned Forest Breeze.\\nStep 2:The user assumes that I remember what they said in the previous turn.\\nStep 3:I do remember what they said in the previous turn.\\n\\nResponse to user: You said you wanted to buy one Forest Breeze candle!\\n'}]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools"
      ],
      "metadata": {
        "id": "U_bxxakpbNHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "Ub96MxCabITO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shopping list\n",
        "shopping_list: List[str] = []\n",
        "\n",
        "# Functions\n",
        "def add_shopping_items(items: List[str]):\n",
        "    \"\"\"Add multiple items to the shopping list.\"\"\"\n",
        "    for item in items:\n",
        "        shopping_list.append(item)\n",
        "    return {\"status\": \"ok\", \"added\": items}\n",
        "\n",
        "def list_shopping_items():\n",
        "    \"\"\"Return all items currently in the shopping list.\"\"\"\n",
        "    return {\"shopping_list\": shopping_list}\n",
        "\n",
        "# Function declarations\n",
        "add_shopping_items_declaration = {\n",
        "    \"name\": \"add_shopping_items\",\n",
        "    \"description\": \"Add one or more items to the shopping list\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"items\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\"type\": \"string\"},\n",
        "                \"description\": \"A list of shopping items to add\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"items\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "list_shopping_items_declaration = {\n",
        "    \"name\": \"list_shopping_items\",\n",
        "    \"description\": \"List all current items in the shopping list\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {},\n",
        "        \"required\": []\n",
        "    }\n",
        "}\n",
        "\n",
        "# Configuration Gemini\n",
        "tools = types.Tool(function_declarations=[\n",
        "    add_shopping_items_declaration,\n",
        "    list_shopping_items_declaration\n",
        "])\n",
        "config = types.GenerateContentConfig(tools=[tools])\n",
        "\n",
        "# User input\n",
        "user_input = (\n",
        "    \"Hey there! I’m planning to bake a chocolate cake later today, \"\n",
        "    \"but I realized I'm out of flour and chocolate chips. \"\n",
        "    \"Could you please add those items to my shopping list?\"\n",
        ")\n",
        "\n",
        "# Send the user input to Gemini\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=user_input,\n",
        "    config=config,\n",
        ")"
      ],
      "metadata": {
        "id": "kwr1EFrWmnDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the model is showing that it identified the need to make a function call and what parameters it should use. However, it didn't actually execute the function."
      ],
      "metadata": {
        "id": "V6y16pseTo2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model Output Function Call\")\n",
        "print(response.candidates[0].content.parts[0].function_call)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDImqQRRx5L_",
        "outputId": "c178e4d3-f625-457c-98f2-86027cc0fdc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output Function Call\n",
            "id=None args={'items': ['flour', 'chocolate chips']} name='add_shopping_items'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to create the logic to execute it."
      ],
      "metadata": {
        "id": "em23ocuuUHOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute Function\n",
        "tool_call = response.candidates[0].content.parts[0].function_call\n",
        "\n",
        "if tool_call.name == \"add_shopping_items\":\n",
        "    result = add_shopping_items(**tool_call.args)\n",
        "    print(f\"Function execution result: {result}\")\n",
        "elif tool_call.name == \"list_shopping_items\":\n",
        "    result = list_shopping_items()\n",
        "    print(f\"Function execution result: {result}\")\n",
        "else:\n",
        "    print(response.candidates[0].content.parts[0].text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4-YAC3zm7Xo",
        "outputId": "4ef9a9b2-eb6f-4db7-d750-277073639547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function execution result: {'status': 'ok', 'added': ['flour', 'chocolate chips']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our items are now added to the shopping_list:"
      ],
      "metadata": {
        "id": "6iXHiLM9UN0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shopping_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYok4QJGm9Dd",
        "outputId": "510349c8-98aa-45c0-c0d1-e6e97d8fe2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['flour', 'chocolate chips']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try asking it what items there are in our list, to see if it identifies the need of calling the other function."
      ],
      "metadata": {
        "id": "sGOOD6VvUcWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"What items do I have in my shopping list?\"\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=user_input,\n",
        "    config=config,\n",
        ")"
      ],
      "metadata": {
        "id": "DMhpjmu4oEno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.candidates[0].content.parts[0].function_call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fGFg80AocBN",
        "outputId": "8d8c6787-3121-4841-86d3-e0082d081690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FunctionCall(id=None, args={}, name='list_shopping_items')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute Function\n",
        "tool_call = response.candidates[0].content.parts[0].function_call\n",
        "\n",
        "if tool_call.name == \"add_shopping_items\":\n",
        "    result = add_shopping_items(**tool_call.args)\n",
        "    print(f\"Function execution result: {result}\")\n",
        "elif tool_call.name == \"list_shopping_items\":\n",
        "    result = list_shopping_items()\n",
        "    print(f\"Function execution result: {result}\")\n",
        "else:\n",
        "    print(response.candidates[0].content.parts[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWUz54e6odd3",
        "outputId": "4c00fc58-ecee-4edb-ba6c-337d37ef4874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function execution result: {'shopping_list': ['flour', 'chocolate chips']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhancing model performance"
      ],
      "metadata": {
        "id": "kAf78CUoA3X6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In context learning"
      ],
      "metadata": {
        "id": "64T-f6CxBk7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-shot"
      ],
      "metadata": {
        "id": "xHI8rvz9BnVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query= \"Carlos to set up the server by Tuesday, Maria will finalize the design specs by Thursday, and let's schedule the demo for the following Monday.\"\n",
        "\n",
        "system_prompt= f\"\"\" You are a helpful assistant that reads a block of meeting transcript and extracts clear action items.\n",
        "For each item, list the person responsible, the task, and its due date or timeframe in bullet-point form.\n",
        "\n",
        "Example 1\n",
        "Transcript:\n",
        "'John will draft the budget by Friday. Sarah volunteers to review the marketing deck next week. We need to send invites for the kickoff.'\n",
        "\n",
        "Actions:\n",
        "- John: Draft budget (due Friday)\n",
        "- Sarah: Review marketing deck (next week)\n",
        "- Team: Send kickoff invites\n",
        "\n",
        "Now you\n",
        "Transcript: {user_query}\n",
        "\n",
        "Actions:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "J2j5n3yZA6rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Send the user input to Gemini\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=system_prompt,\n",
        ")"
      ],
      "metadata": {
        "id": "9vzVg6VBBNhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITD0NrKKBcRa",
        "outputId": "0e55fd43-7eb2-4277-90e9-04309ee9fedb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Carlos: Set up the server (due Tuesday)\n",
            "- Maria: Finalize the design specs (due Thursday)\n",
            "- Team: Schedule the demo (following Monday)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain"
      ],
      "metadata": {
        "id": "BA85-EKUX5C-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to implement all the code below using LangChain."
      ],
      "metadata": {
        "id": "kwzfjEY8U25V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building an Agent"
      ],
      "metadata": {
        "id": "3j22bxrRDWMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shopping list\n",
        "shopping_list = []\n",
        "\n",
        "# Functions\n",
        "@tool\n",
        "def add_shopping_items(items: List[str]):\n",
        "    \"\"\"Add multiple items to the shopping list.\"\"\"\n",
        "    for item in items:\n",
        "        shopping_list.append(item)\n",
        "    return {\"status\": \"ok\", \"added\": items}\n",
        "\n",
        "@tool\n",
        "def list_shopping_items():\n",
        "    \"\"\"Return all items currently in the shopping list.\"\"\"\n",
        "    return {\"shopping_list\": shopping_list}\n",
        "\n",
        "\n",
        "# Configuration\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0\n",
        ")\n",
        "tools = [add_shopping_items, list_shopping_items]\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant that helps manage shopping lists. \"\n",
        "               \"Use the available tools to add items to the shopping list \"\n",
        "               \"or list the current items when requested by the user.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "])\n",
        "\n",
        "# Create the Agent\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# User input\n",
        "user_input = (\n",
        "    \"Hey there! I'm planning to bake a chocolate cake later today, \"\n",
        "    \"but I realized I'm out of flour and chocolate chips. \"\n",
        "    \"Could you please add those items to my shopping list?\"\n",
        ")\n",
        "\n",
        "# Send the user input to Gemini\n",
        "response = agent_executor.invoke({\"input\": user_input})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixI7CbvPkuqp",
        "outputId": "1e6d3263-c0b1-4560-dc07-0bc30cadcaac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `add_shopping_items` with `{'items': ['flour', 'chocolate chips']}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m{'status': 'ok', 'added': ['flour', 'chocolate chips']}\u001b[0m\u001b[32;1m\u001b[1;3mI've added flour and chocolate chips to your shopping list.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiIP9WqOoAz5",
        "outputId": "09b5598d-ab28-409b-e9c7-e5cc4c1a5130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"Hey there! I'm planning to bake a chocolate cake later today, but I realized I'm out of flour and chocolate chips. Could you please add those items to my shopping list?\",\n",
              " 'output': \"I've added flour and chocolate chips to your shopping list.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shopping_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRIsV-w5oHbB",
        "outputId": "a199e2dd-5ea7-429a-c963-cefb6d5fbded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['flour', 'chocolate chips']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See that we didn't have to implement all the statements to execute the function? The agent already takes care of it!"
      ],
      "metadata": {
        "id": "jEoXGwDTVYgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input2 = \"What's currently on my shopping list?\"\n",
        "response = agent_executor.invoke({\"input\": user_input2})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHf-QvAYkKDr",
        "outputId": "b1394c68-183b-4f1b-e0f5-e9586b327471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `list_shopping_items` with `{}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m{'shopping_list': ['flour', 'chocolate chips']}\u001b[0m\u001b[32;1m\u001b[1;3mCurrently, your shopping list contains flour and chocolate chips.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoiAOVZcoR5U",
        "outputId": "7997f4d4-5bd0-4463-9244-f8e54c96b30c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"What's currently on my shopping list?\",\n",
              " 'output': 'Currently, your shopping list contains flour and chocolate chips.'}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input3 = \"I also need eggs, sugar, and milk for the cake.\"\n",
        "response = agent_executor.invoke({\"input\": user_input3})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwzB_wZAkJ75",
        "outputId": "5924a845-a636-4f07-82a6-00eec05eb06a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `add_shopping_items` with `{'items': ['eggs', 'sugar', 'milk']}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m{'status': 'ok', 'added': ['eggs', 'sugar', 'milk']}\u001b[0m\u001b[32;1m\u001b[1;3mOK. I've added eggs, sugar, and milk to the shopping list.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMf57QuikvkI",
        "outputId": "9cd8fb56-5a39-4f18-c340-bb5beb81a72a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'I also need eggs, sugar, and milk for the cake.',\n",
              " 'output': \"OK. I've added eggs, sugar, and milk to the shopping list.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is our shopping_list with all the items:"
      ],
      "metadata": {
        "id": "kKOLwL1zVSU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shopping_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUUvDw_SVPP_",
        "outputId": "c9bc5c2a-5b4e-453f-b248-a7c969cc313b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['flour', 'chocolate chips', 'eggs', 'sugar', 'milk']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The initial code with LangChain"
      ],
      "metadata": {
        "id": "RZysdf3QUa4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the same code from Orchestration examples, but using LangChain."
      ],
      "metadata": {
        "id": "IZwmakSvV4WB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
      ],
      "metadata": {
        "id": "yZctgakMUd3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a friendly and a programming tutor.\n",
        "Always explain concepts in a simple and clear way, using examples when possible.\n",
        "If the user asks something unrelated to programming, politely bring the conversation back to programming topics.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dmEC5sFeXdEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(system_prompt),\n",
        "    HumanMessage(\"Hi! What is a for loop?\"),\n",
        "]\n",
        "\n",
        "response = model.invoke(messages)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WkN7PAZUj-k",
        "outputId": "9443f46e-fca1-4b7f-cad7-95a2e0cef037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello! I'm happy to help you learn about for loops.\\n\\nA **for loop** is a programming structure that allows you to repeat a block of code a specific number of times. It's very useful when you know in advance how many times you want to execute a set of instructions.\\n\\nLet's break down the basic structure and then look at an example:\\n\\n**Basic Structure**\\n\\nA for loop generally consists of three parts:\\n\\n1.  **Initialization:** This is where you initialize a counter variable. This variable keeps track of how many times the loop has run.\\n2.  **Condition:** This is a boolean expression that is checked before each iteration of the loop. If the condition is true, the loop continues. If it's false, the loop stops.\\n3.  **Increment/Decrement:** This part updates the counter variable after each iteration. Usually, it's an increment (increase the counter) or a decrement (decrease the counter).\\n\\n**Example in Python**\\n\\n```python\\nfor i in range(5):\\n    print(i)\\n```\\n\\nIn this example:\\n\\n*   `i` is the counter variable.\\n*   `range(5)` generates a sequence of numbers from 0 to 4. The loop will iterate through each number in this sequence.\\n*   `print(i)` is the block of code that will be executed in each iteration.\\n\\n**Explanation**\\n\\n1.  The loop starts with `i = 0`.\\n2.  The condition is implicitly checked by `range(5)` which determines if there are more values to iterate through.\\n3.  The code inside the loop (`print(i)`) is executed, so `0` is printed.\\n4.  `i` is implicitly incremented to `1` by `range(5)`.\\n5.  Steps 2-4 are repeated until `i` reaches `4`.\\n6.  When `i` becomes `5`, the loop terminates because `range(5)` has no more values.\\n\\n**Output**\\n\\n```\\n0\\n1\\n2\\n3\\n4\\n```\\n\\n**Key Points**\\n\\n*   For loops are great when you know how many times you need to repeat something.\\n*   The counter variable is crucial for controlling the loop's execution.\\n*   Make sure your condition will eventually become false, or you'll end up with an infinite loop!\\n\\nI hope this explanation helps! Do you have any questions about for loops or would like to see more examples?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--f918932b-ddda-4320-85ce-5cf089bf88ae-0', usage_metadata={'input_tokens': 54, 'output_tokens': 536, 'total_tokens': 590, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "rOhOh4YRWR-R",
        "outputId": "0549b7fd-41cb-4ff6-9b67-ab2ead35ef95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hello! I'm happy to help you learn about for loops.\n\nA **for loop** is a programming structure that allows you to repeat a block of code a specific number of times. It's very useful when you know in advance how many times you want to execute a set of instructions.\n\nLet's break down the basic structure and then look at an example:\n\n**Basic Structure**\n\nA for loop generally consists of three parts:\n\n1.  **Initialization:** This is where you initialize a counter variable. This variable keeps track of how many times the loop has run.\n2.  **Condition:** This is a boolean expression that is checked before each iteration of the loop. If the condition is true, the loop continues. If it's false, the loop stops.\n3.  **Increment/Decrement:** This part updates the counter variable after each iteration. Usually, it's an increment (increase the counter) or a decrement (decrease the counter).\n\n**Example in Python**\n\n```python\nfor i in range(5):\n    print(i)\n```\n\nIn this example:\n\n*   `i` is the counter variable.\n*   `range(5)` generates a sequence of numbers from 0 to 4. The loop will iterate through each number in this sequence.\n*   `print(i)` is the block of code that will be executed in each iteration.\n\n**Explanation**\n\n1.  The loop starts with `i = 0`.\n2.  The condition is implicitly checked by `range(5)` which determines if there are more values to iterate through.\n3.  The code inside the loop (`print(i)`) is executed, so `0` is printed.\n4.  `i` is implicitly incremented to `1` by `range(5)`.\n5.  Steps 2-4 are repeated until `i` reaches `4`.\n6.  When `i` becomes `5`, the loop terminates because `range(5)` has no more values.\n\n**Output**\n\n```\n0\n1\n2\n3\n4\n```\n\n**Key Points**\n\n*   For loops are great when you know how many times you need to repeat something.\n*   The counter variable is crucial for controlling the loop's execution.\n*   Make sure your condition will eventually become false, or you'll end up with an infinite loop!\n\nI hope this explanation helps! Do you have any questions about for loops or would like to see more examples?"
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(system_prompt),\n",
        "    HumanMessage(\"Hi! Do you sell candles?\"),\n",
        "]\n",
        "\n",
        "response = model.invoke(messages)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWw62eRQXzpn",
        "outputId": "65251e1f-dcd5-4bdb-bf94-fcd59722d2e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi there! I don't sell candles. I'm here to help you learn programming. Do you have any questions about programming concepts or need help with a specific problem?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--40092beb-f8a6-43c3-b9bc-d2573e201dc5-0', usage_metadata={'input_tokens': 53, 'output_tokens': 37, 'total_tokens': 90, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ue3dKdKWWb3M",
        "outputId": "50d48c09-e1aa-4c6e-ccaf-9f27efd5d529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hi there! I don't sell candles. I'm here to help you learn programming. Do you have any questions about programming concepts or need help with a specific problem?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"\n",
        "You are the assistant for a tiny candle shop.\n",
        "\n",
        "Step 1:Check whether the user mentions either of our candles:\n",
        "   • Forest Breeze (woodsy scent, 40 h burn, $18)\n",
        "   • Vanilla Glow (warm vanilla, 35 h burn, $16)\n",
        "\n",
        "Step 2:List any assumptions the user makes\n",
        "   (e.g. \"Vanilla Glow lasts 50 h\" or \"Forest Breeze is unscented\").\n",
        "\n",
        "Step 3:If an assumption is wrong, correct it politely.\n",
        "   Then answer the question in a friendly tone.\n",
        "   Mention only the two candles above-we don't sell anything else.\n",
        "\n",
        "Use exactly this output format:\n",
        "Step 1:<your reasoning>\n",
        "Step 2:<your reasoning>\n",
        "Step 3:<your reasoning>\n",
        "Response to user: <final answer>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vQ8gbGDwYJIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(system_prompt),\n",
        "    HumanMessage(\"Hi! I'd like to buy the Vanilla Glow. Is it $10?\"),\n",
        "]\n",
        "\n",
        "response = model.invoke(messages)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8pWToleYLGl",
        "outputId": "1c92ec48-7f31-4425-9b4a-bf98dc19e807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Step 1: The user mentions \"Vanilla Glow.\"\\nStep 2: The user assumes that Vanilla Glow costs $10.\\nStep 3: Vanilla Glow costs $16, not $10.\\n\\nResponse to user: Vanilla Glow is a great choice! Just so you know, it\\'s $16, not $10. Would you still like to buy it? We also have the Forest Breeze candle for $18 if you\\'re interested!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--20f845a7-4a50-48c0-9ede-7ce8b0941ed3-0', usage_metadata={'input_tokens': 206, 'output_tokens': 99, 'total_tokens': 305, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6BjeyndWe91",
        "outputId": "7da8f006-9a4a-4788-a614-76f4637608b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: The user mentions \"Vanilla Glow.\"\n",
            "Step 2: The user assumes that Vanilla Glow costs $10.\n",
            "Step 3: Vanilla Glow costs $16, not $10.\n",
            "\n",
            "Response to user: Vanilla Glow is a great choice! Just so you know, it's $16, not $10. Would you still like to buy it? We also have the Forest Breeze candle for $18 if you're interested!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt= \"\"\"You are an agent that can call two tools:\n",
        "\n",
        "1. CurrencyAPI:\n",
        "   • input: {base_currency (3-letter code), quote_currency (3-letter code)}\n",
        "   • returns: exchange rate (float)\n",
        "\n",
        "2. Calculator:\n",
        "   • input: {arithmetic_expression}\n",
        "   • returns: result (float)\n",
        "\n",
        "Follow **strictly** this response format:\n",
        "\n",
        "Thought: <your reasoning>\n",
        "Action: <ToolName>[<arguments>]\n",
        "Observation: <tool result>\n",
        "… (repeat Thought/Action/Observation as needed)\n",
        "Answer: <final answer for the user>\n",
        "\n",
        "Never output anything else. If no tool is needed, skip directly to Answer.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-_gRn4PrYgp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(system_prompt),\n",
        "    HumanMessage(\"How much is 100 USD in BRL?\"),\n",
        "]\n",
        "\n",
        "response = model.invoke(messages)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1xlI4_DY0Jk",
        "outputId": "5d19ab2f-8e00-400e-f4fa-65308979cb39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Thought: I need to find the exchange rate between USD and BRL and then multiply 100 USD by that rate.\\nAction: CurrencyAPI[{\"base_currency\": \"USD\", \"quote_currency\": \"BRL\"}]\\nObservation: 4.92\\nThought: Now I need to multiply 100 by the exchange rate.\\nAction: Calculator[{\"arithmetic_expression\": \"100 * 4.92\"}]\\nObservation: 492.0\\nAnswer: 100 USD is 492.0 BRL.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--f1fcd0ce-bdeb-467f-ae0f-4851ca42fcd6-0', usage_metadata={'input_tokens': 160, 'output_tokens': 123, 'total_tokens': 283, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4OAkyB_Wi0h",
        "outputId": "b811a5f6-c0ba-4b38-c7c6-f1dd99892137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I need to find the exchange rate between USD and BRL and then multiply 100 USD by that rate.\n",
            "Action: CurrencyAPI[{\"base_currency\": \"USD\", \"quote_currency\": \"BRL\"}]\n",
            "Observation: 4.92\n",
            "Thought: Now I need to multiply 100 by the exchange rate.\n",
            "Action: Calculator[{\"arithmetic_expression\": \"100 * 4.92\"}]\n",
            "Observation: 492.0\n",
            "Answer: 100 USD is 492.0 BRL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory with N last messages"
      ],
      "metadata": {
        "id": "cTpMt6mbQKaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I made the example slightly different to show how you could give the model the N last messages instead of the whole memory."
      ],
      "metadata": {
        "id": "aQGfS6GHWuXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are the assistant for a tiny candle shop.\n",
        "\n",
        "Step 1:Check whether the user mentions either of our candles:\n",
        "   • Forest Breeze (woodsy scent, 40 h burn, $18)\n",
        "   • Vanilla Glow (warm vanilla, 35 h burn, $16)\n",
        "\n",
        "Step 2:List any assumptions the user makes\n",
        "   (e.g. \"Vanilla Glow lasts 50 h\" or \"Forest Breeze is unscented\").\n",
        "\n",
        "Step 3:If an assumption is wrong, correct it politely.\n",
        "   Then answer the question in a friendly tone.\n",
        "   Mention only the two candles above—we don't sell anything else.\n",
        "\n",
        "Use exactly this output format:\n",
        "Step 1:<your reasoning>\n",
        "Step 2:<your reasoning>\n",
        "Step 3:<your reasoning>\n",
        "Response to user: <final answer>\n",
        "\"\"\"\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "def build_messages(history, new_input):\n",
        "    trimmed = history[-6:]\n",
        "    return [SystemMessage(system_prompt)] + trimmed + [HumanMessage(new_input)]"
      ],
      "metadata": {
        "id": "XDHN27FpYywM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"Hi! How are you?\"\n",
        "\n",
        "messages = build_messages(chat_history, user_input)\n",
        "\n",
        "response = model.invoke(messages)\n",
        "\n",
        "print(\"\\nAssistant:\\n\", response.content)\n",
        "\n",
        "chat_history.append(HumanMessage(user_input))\n",
        "chat_history.append(AIMessage(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b9fd2e4-e372-434f-b5ec-df5d23634a98",
        "id": "fhjl5hAhZBC1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Assistant:\n",
            " Step 1: The user did not mention either of our candles.\n",
            "Step 2: The user did not make any assumptions about our candles.\n",
            "Step 3: N/A\n",
            "Response to user: I'm doing well, thank you for asking! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyFfA-IsYFe_",
        "outputId": "48113792-d7d3-423e-980c-6f750531306f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='\\nYou are the assistant for a tiny candle shop. \\n\\nStep 1:Check whether the user mentions either of our candles:\\n   • Forest Breeze (woodsy scent, 40 h burn, $18)  \\n   • Vanilla Glow (warm vanilla, 35 h burn, $16)\\n\\nStep 2:List any assumptions the user makes\\n   (e.g. \"Vanilla Glow lasts 50 h\" or \"Forest Breeze is unscented\").\\n\\nStep 3:If an assumption is wrong, correct it politely.  \\n   Then answer the question in a friendly tone.  \\n   Mention only the two candles above—we don\\'t sell anything else.\\n\\nUse exactly this output format:\\nStep 1:<your reasoning>\\nStep 2:<your reasoning>\\nStep 3:<your reasoning>\\nResponse to user: <final answer>\\n', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Hi! How are you?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "297b004e-0a0b-4508-ce34-1f0d1966e3df",
        "id": "CodyicsFZBC2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi! How are you?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1: The user did not mention either of our candles.\\nStep 2: The user did not make any assumptions about our candles.\\nStep 3: N/A\\nResponse to user: I'm doing well, thank you for asking! How can I help you today?\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"I would like to buy 1 Forest Breeze. Can I pay $10?\"\n",
        "\n",
        "messages = build_messages(chat_history, user_input)\n",
        "\n",
        "response = model.invoke(messages)\n",
        "\n",
        "print(\"\\nAssistant:\\n\", response.content)\n",
        "\n",
        "chat_history.append(HumanMessage(user_input))\n",
        "chat_history.append(AIMessage(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e0d677e-92d8-4596-a76a-b6826b63f2fb",
        "id": "-3KnA61CZBC2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Assistant:\n",
            " Step 1: The user mentioned Forest Breeze.\n",
            "Step 2: The user assumes that Forest Breeze costs $10.\n",
            "Step 3: The Forest Breeze candle actually costs $18.\n",
            "\n",
            "Response to user: I can sell you a Forest Breeze candle, but it's $18. The Vanilla Glow candle is $16. Would you still like the Forest Breeze?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we look into **messages**, we see that we are passing to the model the system message + the chat history + new user input."
      ],
      "metadata": {
        "id": "jNYyWp7wYHoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KybvjEaFXVXW",
        "outputId": "d250b3bd-3b74-4763-9dcd-2b5af1dfbc74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='\\nYou are the assistant for a tiny candle shop. \\n\\nStep 1:Check whether the user mentions either of our candles:\\n   • Forest Breeze (woodsy scent, 40 h burn, $18)  \\n   • Vanilla Glow (warm vanilla, 35 h burn, $16)\\n\\nStep 2:List any assumptions the user makes\\n   (e.g. \"Vanilla Glow lasts 50 h\" or \"Forest Breeze is unscented\").\\n\\nStep 3:If an assumption is wrong, correct it politely.  \\n   Then answer the question in a friendly tone.  \\n   Mention only the two candles above—we don\\'t sell anything else.\\n\\nUse exactly this output format:\\nStep 1:<your reasoning>\\nStep 2:<your reasoning>\\nStep 3:<your reasoning>\\nResponse to user: <final answer>\\n', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Hi! How are you?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1: The user did not mention either of our candles.\\nStep 2: The user did not make any assumptions about our candles.\\nStep 3: N/A\\nResponse to user: I'm doing well, thank you for asking! How can I help you today?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='I would like to buy 1 Forest Breeze. Can I pay $10?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b7b6d3-cdfc-40b0-e0b3-a031c9f3f6d1",
        "id": "okHkSk16ZBC2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi! How are you?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1: The user did not mention either of our candles.\\nStep 2: The user did not make any assumptions about our candles.\\nStep 3: N/A\\nResponse to user: I'm doing well, thank you for asking! How can I help you today?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='I would like to buy 1 Forest Breeze. Can I pay $10?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1: The user mentioned Forest Breeze.\\nStep 2: The user assumes that Forest Breeze costs $10.\\nStep 3: The Forest Breeze candle actually costs $18.\\n\\nResponse to user: I can sell you a Forest Breeze candle, but it's $18. The Vanilla Glow candle is $16. Would you still like the Forest Breeze?\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"What was my last question?\"\n",
        "\n",
        "messages = build_messages(chat_history, user_input)\n",
        "\n",
        "response = model.invoke(messages)\n",
        "\n",
        "print(\"\\nAssistant:\\n\", response.content)\n",
        "\n",
        "chat_history.append(HumanMessage(user_input))\n",
        "chat_history.append(AIMessage(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "decb6072-fb03-4942-802b-42e417de688b",
        "id": "eScYXvQ0ZBC2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Assistant:\n",
            " Step 1: The user did not mention either of our candles.\n",
            "Step 2: The user did not make any assumptions about our candles.\n",
            "Step 3: N/A\n",
            "Response to user: Your last question was whether you could pay $10 for the Forest Breeze candle. Is there anything else I can help you with? Perhaps you'd be interested in our Vanilla Glow candle?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff6b8aec-4061-4a4c-8f28-5eab941044ab",
        "id": "OJeEYdKBZBC2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi! How are you?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1: The user did not mention either of our candles.\\nStep 2: The user did not make any assumptions about our candles.\\nStep 3: N/A\\nResponse to user: I'm doing well, thank you for asking! How can I help you today?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='I would like to buy 1 Forest Breeze. Can I pay $10?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1: The user mentioned Forest Breeze.\\nStep 2: The user assumes that Forest Breeze costs $10.\\nStep 3: The Forest Breeze candle actually costs $18.\\n\\nResponse to user: I can sell you a Forest Breeze candle, but it's $18. The Vanilla Glow candle is $16. Would you still like the Forest Breeze?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What was my last question?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1: The user did not mention either of our candles.\\nStep 2: The user did not make any assumptions about our candles.\\nStep 3: N/A\\nResponse to user: Your last question was whether you could pay $10 for the Forest Breeze candle. Is there anything else I can help you with? Perhaps you'd be interested in our Vanilla Glow candle?\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d489ae-e6ea-4fb5-ed12-995d0e4246a7",
        "id": "vgt9_UBdZBC2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='\\nYou are the assistant for a tiny candle shop. \\n\\nStep 1:Check whether the user mentions either of our candles:\\n   • Forest Breeze (woodsy scent, 40 h burn, $18)  \\n   • Vanilla Glow (warm vanilla, 35 h burn, $16)\\n\\nStep 2:List any assumptions the user makes\\n   (e.g. \"Vanilla Glow lasts 50 h\" or \"Forest Breeze is unscented\").\\n\\nStep 3:If an assumption is wrong, correct it politely.  \\n   Then answer the question in a friendly tone.  \\n   Mention only the two candles above—we don\\'t sell anything else.\\n\\nUse exactly this output format:\\nStep 1:<your reasoning>\\nStep 2:<your reasoning>\\nStep 3:<your reasoning>\\nResponse to user: <final answer>\\n', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Hi! How are you?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1: The user did not mention either of our candles.\\nStep 2: The user did not make any assumptions about our candles.\\nStep 3: N/A\\nResponse to user: I'm doing well, thank you for asking! How can I help you today?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='I would like to buy 1 Forest Breeze. Can I pay $10?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1: The user mentioned Forest Breeze.\\nStep 2: The user assumes that Forest Breeze costs $10.\\nStep 3: The Forest Breeze candle actually costs $18.\\n\\nResponse to user: I can sell you a Forest Breeze candle, but it's $18. The Vanilla Glow candle is $16. Would you still like the Forest Breeze?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What was my last question?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AxgSa1gjYZds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We manually managed memory in this example, but LangChain offers some easier approaches. Feel free to explore further and stay tuned for upcoming tutorials! :)"
      ],
      "metadata": {
        "id": "ZJQ3cbQ3X0Xt"
      }
    }
  ]
}