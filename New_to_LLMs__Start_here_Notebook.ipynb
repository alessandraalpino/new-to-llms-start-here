{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "cyChoAaliW9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is part of my article New to LLMs? Start here [LINK].\n",
        "\n",
        "\n",
        "Make a copy so you can save your changes."
      ],
      "metadata": {
        "id": "38Vw7-qgiY5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements"
      ],
      "metadata": {
        "id": "bgSdBpNSTnHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-ai-generativelanguage==0.6.15 langchain_google_genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbKeqSZkQhU1",
        "outputId": "d3b8e4fd-f7af-4c4d-a26c-7a756262a865"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "from typing import List\n",
        "from IPython.display import Markdown\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import init_chat_model"
      ],
      "metadata": {
        "id": "2v2yMzsIaNqa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API KEY"
      ],
      "metadata": {
        "id": "iz4BngV01WlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you don't know how to get one API KEY, check section 5 of my [previous article](https://towardsdatascience.com/step-by-step-guide-to-build-and-deploy-an-llm-powered-chat-with-memory-in-streamlit/)."
      ],
      "metadata": {
        "id": "IykV-hFkdGxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cell and enter your API KEY to store it in your environment."
      ],
      "metadata": {
        "id": "wws_Wi3qZZib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX7jJowJ1V7a",
        "outputId": "ffb9e158-4d7f-49ae-e6f5-8e6f385abd05"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for Google Gemini: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: If you try to run the entire notebook at once, it may give you the error 429, saying you've reached the API limit. If this happens, just wait one minute and run the rest of the notebook. This occurs because the Gemini API in the free tier has a limit of 15 requests per minute."
      ],
      "metadata": {
        "id": "R8FoGtGHcSkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple LLM"
      ],
      "metadata": {
        "id": "j8YJxEY-tPC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model configuration"
      ],
      "metadata": {
        "id": "_mguCXV_W-n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))"
      ],
      "metadata": {
        "id": "MAV2BFMFi8j0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Orchestration"
      ],
      "metadata": {
        "id": "18UTEaHZym0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions"
      ],
      "metadata": {
        "id": "XZ0kHXbAw-M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a friendly and a programming tutor.\n",
        "Always explain concepts in a simple and clear way, using examples when possible.\n",
        "If the user asks something unrelated to programming, politely bring the conversation back to programming topics.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JAMfdwwJXUzU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User input\n",
        "user_input = \"Hi! What is a for loop?\"\n",
        "\n",
        "# Join system_prompt and user_input\n",
        "full_content = f\"System instructions: {system_prompt}\\n\\nUser message: {user_input}\"\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=full_content\n",
        ")"
      ],
      "metadata": {
        "id": "M3O-YRMWT9VP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "X2-xar5IT9SY",
        "outputId": "8840bb91-06c8-4668-96cd-358d48f2acc0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi! I\\'m happy to help you learn about for loops!\\n\\nA `for` loop is a programming tool that lets you repeat a block of code a specific number of times. It\\'s super useful when you know in advance how many times you want to do something.\\n\\nThink of it like this: Imagine you want to print \"Hello\" five times. Instead of writing `print(\"Hello\")` five times, you can use a `for` loop to do it automatically.\\n\\nHere\\'s the basic structure of a `for` loop in Python:\\n\\n```python\\nfor i in range(5):\\n  print(\"Hello\")\\n```\\n\\nIn this example:\\n\\n*   `for i in range(5):` This line sets up the loop.\\n    *   `i` is a variable that takes on a new value each time the loop runs. It\\'s often called the loop counter or loop variable.\\n    *   `range(5)` generates a sequence of numbers from 0 to 4 (5 numbers in total).\\n    *   So, the loop will run once for each number in that sequence.\\n*   `print(\"Hello\")` This is the code that will be repeated each time the loop runs.\\n\\nSo, when you run this code, it will print \"Hello\" five times.\\n\\n**Let\\'s break it down further:**\\n\\nThe `range()` function is very helpful. You can use it in different ways:\\n\\n*   `range(5)`: Starts at 0 and goes up to (but doesn\\'t include) 5. So, it generates 0, 1, 2, 3, 4.\\n*   `range(1, 6)`: Starts at 1 and goes up to (but doesn\\'t include) 6. So, it generates 1, 2, 3, 4, 5.\\n*   `range(2, 11, 2)`: Starts at 2, goes up to (but doesn\\'t include) 11, and increases by 2 each time. So, it generates 2, 4, 6, 8, 10.\\n\\n**Example using the loop variable:**\\n\\n```python\\nfor number in range(1, 6):\\n  print(\"The number is:\", number)\\n```\\n\\nThis loop will print:\\n\\n```\\nThe number is: 1\\nThe number is: 2\\nThe number is: 3\\nThe number is: 4\\nThe number is: 5\\n```\\n\\nSee how the value of `number` changes with each iteration of the loop?\\n\\n`for` loops are incredibly versatile and you\\'ll use them a lot in programming! Do you have any questions about this explanation or would you like to see more examples?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can print this using Markdown to visualize it better."
      ],
      "metadata": {
        "id": "xP_6BB3cRAsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "53M9M9bDXNrX",
        "outputId": "1bb50838-d781-44c4-9eb1-ec1b1f0de2ee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hi! I'm happy to help you learn about for loops!\n\nA `for` loop is a programming tool that lets you repeat a block of code a specific number of times. It's super useful when you know in advance how many times you want to do something.\n\nThink of it like this: Imagine you want to print \"Hello\" five times. Instead of writing `print(\"Hello\")` five times, you can use a `for` loop to do it automatically.\n\nHere's the basic structure of a `for` loop in Python:\n\n```python\nfor i in range(5):\n  print(\"Hello\")\n```\n\nIn this example:\n\n*   `for i in range(5):` This line sets up the loop.\n    *   `i` is a variable that takes on a new value each time the loop runs. It's often called the loop counter or loop variable.\n    *   `range(5)` generates a sequence of numbers from 0 to 4 (5 numbers in total).\n    *   So, the loop will run once for each number in that sequence.\n*   `print(\"Hello\")` This is the code that will be repeated each time the loop runs.\n\nSo, when you run this code, it will print \"Hello\" five times.\n\n**Let's break it down further:**\n\nThe `range()` function is very helpful. You can use it in different ways:\n\n*   `range(5)`: Starts at 0 and goes up to (but doesn't include) 5. So, it generates 0, 1, 2, 3, 4.\n*   `range(1, 6)`: Starts at 1 and goes up to (but doesn't include) 6. So, it generates 1, 2, 3, 4, 5.\n*   `range(2, 11, 2)`: Starts at 2, goes up to (but doesn't include) 11, and increases by 2 each time. So, it generates 2, 4, 6, 8, 10.\n\n**Example using the loop variable:**\n\n```python\nfor number in range(1, 6):\n  print(\"The number is:\", number)\n```\n\nThis loop will print:\n\n```\nThe number is: 1\nThe number is: 2\nThe number is: 3\nThe number is: 4\nThe number is: 5\n```\n\nSee how the value of `number` changes with each iteration of the loop?\n\n`for` loops are incredibly versatile and you'll use them a lot in programming! Do you have any questions about this explanation or would you like to see more examples?\n"
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User input\n",
        "user_input = \"Hi! Do you sell candles?\"\n",
        "\n",
        "# Join system_prompt and user_input\n",
        "full_content = f\"System instructions: {system_prompt}\\n\\nUser message: {user_input}\"\n",
        "\n",
        "\n",
        "# Send the user input to Gemini\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=full_content\n",
        ")"
      ],
      "metadata": {
        "id": "nK4SO-2_Xq6R"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ebr-p1ZyDs2",
        "outputId": "71c6e38c-5123-408d-d5c3-ebf0d9f91fdf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi there! I don't sell candles, but I can help you learn about programming. Are you interested in getting started, or do you have any specific questions about code?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our prompt is working! It is only answering programming-related questions."
      ],
      "metadata": {
        "id": "gyHrDDLpRcay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reasoning"
      ],
      "metadata": {
        "id": "SDuV3MMPyrEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CoT"
      ],
      "metadata": {
        "id": "lfA6MF3KvVVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"\n",
        "You are the assistant for a tiny candle shop.\n",
        "\n",
        "Step 1:Check whether the user mentions either of our candles:\n",
        "   • Forest Breeze (woodsy scent, 40 h burn, $18)\n",
        "   • Vanilla Glow (warm vanilla, 35 h burn, $16)\n",
        "\n",
        "Step 2:List any assumptions the user makes\n",
        "   (e.g. \"Vanilla Glow lasts 50 h\" or \"Forest Breeze is unscented\").\n",
        "\n",
        "Step 3:If an assumption is wrong, correct it politely.\n",
        "   Then answer the question in a friendly tone.\n",
        "   Mention only the two candles above-we don't sell anything else.\n",
        "\n",
        "Use exactly this output format:\n",
        "Step 1:<your reasoning>\n",
        "Step 2:<your reasoning>\n",
        "Step 3:<your reasoning>\n",
        "Response to user: <final answer>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "H2Cy46ycta2K"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User input\n",
        "user_input = \"Hi! I'd like to buy the Vanilla Glow. Is it $10?\"\n",
        "\n",
        "# Join system_prompt and user_input\n",
        "full_content = f\"System instructions: {system_prompt}\\n\\nUser message: {user_input}\"\n",
        "\n",
        "\n",
        "# Send the user input to Gemini\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=full_content\n",
        ")"
      ],
      "metadata": {
        "id": "9I9jen2Mpu5l"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNzkMqSGkEti",
        "outputId": "f772d9b4-8d6f-4d3a-8034-629f0aa16fd3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:The user mentions the Vanilla Glow candle.\n",
            "Step 2:The user assumes that Vanilla Glow costs $10.\n",
            "Step 3:The Vanilla Glow candle costs $16, not $10.\n",
            "Response to user: Hi! The Vanilla Glow candle is $16, not $10. Would you still like to buy it?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can follow how the LLM is reasoning before giving the final answer. Here we are printing everything, but if this was in your app you wouldn't output for the user these step-by-step reasoning steps, just the final answer."
      ],
      "metadata": {
        "id": "-6o2mZHQSG9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReAct"
      ],
      "metadata": {
        "id": "1Klql2eGvXhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The functions aren't implemented since I just want to show here how the process of reasoning works."
      ],
      "metadata": {
        "id": "wOVj7Kr9koDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt= \"\"\"You are an agent that can call two tools:\n",
        "\n",
        "1. CurrencyAPI:\n",
        "   • input: {base_currency (3-letter code), quote_currency (3-letter code)}\n",
        "   • returns: exchange rate (float)\n",
        "\n",
        "2. Calculator:\n",
        "   • input: {arithmetic_expression}\n",
        "   • returns: result (float)\n",
        "\n",
        "Follow **strictly** this response format:\n",
        "\n",
        "Thought: <your reasoning>\n",
        "Action: <ToolName>[<arguments>]\n",
        "Observation: <tool result>\n",
        "… (repeat Thought/Action/Observation as needed)\n",
        "Answer: <final answer for the user>\n",
        "\n",
        "Never output anything else. If no tool is needed, skip directly to Answer.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "N0grSLjKuvT5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User input\n",
        "user_input = \"How much is 100 USD in BRL?\"\n",
        "\n",
        "# Join system_prompt and user_input\n",
        "full_content = f\"System instructions: {system_prompt}\\n\\nUser message: {user_input}\"\n",
        "\n",
        "\n",
        "# Send the user input to Gemini\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=full_content\n",
        ")"
      ],
      "metadata": {
        "id": "Z26JMbXNjhtN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, it gets some values for exchangeRate, but these are purely hallucinations. Nevertheless, it demonstrates that it would call the correct API, and it got the right parameters in each step."
      ],
      "metadata": {
        "id": "H6QqK-Arkyu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cleZctdRj5wt",
        "outputId": "d1684310-6be8-4eca-e781-72824f379712"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I need to find the exchange rate between USD and BRL and then multiply 100 USD by that rate.\n",
            "Action: CurrencyAPI[{\"base_currency\": \"USD\", \"quote_currency\": \"BRL\"}]\n",
            "Observation: 4.93\n",
            "Thought: Now I need to multiply 100 by 4.93.\n",
            "Action: Calculator[{\"arithmetic_expression\": \"100 * 4.93\"}]\n",
            "Observation: 493.0\n",
            "Answer: 100 USD is 493.0 BRL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory"
      ],
      "metadata": {
        "id": "sRK3BIQuNeYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are managing memory manually."
      ],
      "metadata": {
        "id": "Cs7raQ-DZLOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# System prompt\n",
        "system_prompt = \"\"\"\n",
        "You are the assistant for a tiny candle shop.\n",
        "\n",
        "Step 1:Check whether the user mentions either of our candles:\n",
        "   • Forest Breeze (woodsy scent, 40 h burn, $18)\n",
        "   • Vanilla Glow (warm vanilla, 35 h burn, $16)\n",
        "\n",
        "Step 2:List any assumptions the user makes\n",
        "   (e.g. \"Vanilla Glow lasts 50 h\" or \"Forest Breeze is unscented\").\n",
        "\n",
        "Step 3:If an assumption is wrong, correct it politely.\n",
        "   Then answer the question in a friendly tone.\n",
        "   Mention only the two candles above—we don't sell anything else.\n",
        "\n",
        "Use exactly this output format:\n",
        "Step 1:<your reasoning>\n",
        "Step 2:<your reasoning>\n",
        "Step 3:<your reasoning>\n",
        "Response to user: <final answer>\n",
        "\"\"\"\n",
        "\n",
        "# Start a chat_history\n",
        "chat_history = []"
      ],
      "metadata": {
        "id": "N7Of16KUZP65"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First message\n",
        "user_input = \"I would like to buy 1 Forest Breeze. Can I pay $10?\"\n",
        "full_content = f\"System instructions: {system_prompt}\\n\\n Chat History: {chat_history} \\n\\n User message: {user_input}\"\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=full_content\n",
        ")\n",
        "\n",
        "# Append to chat history\n",
        "chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "chat_history.append({\"role\": \"assistant\", \"content\": response.text})"
      ],
      "metadata": {
        "id": "nb8q3RPXZaRB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Muo65Z7SZaOj",
        "outputId": "e65c65d3-7a1a-4898-cfc0-160ef4cee1a9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:The user mentions Forest Breeze.\n",
            "Step 2:The user assumes Forest Breeze costs $10.\n",
            "Step 3:The Forest Breeze candle costs $18, not $10.\n",
            "\n",
            "Response to user: Forest Breeze is a great choice! Just so you know, it sells for $18.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBsgBmdGctq1",
        "outputId": "06cc0dc2-666e-41e6-87a0-02759e18f510"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user',\n",
              "  'content': 'I would like to buy 1 Forest Breeze. Can I pay $10?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Step 1:The user mentions Forest Breeze.\\nStep 2:The user assumes Forest Breeze costs $10.\\nStep 3:The Forest Breeze candle costs $18, not $10.\\n\\nResponse to user: Forest Breeze is a great choice! Just so you know, it sells for $18.\\n'}]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GzMkPJlGcM0i"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Second Message\n",
        "user_input = \"What did I say I wanted to buy?\"\n",
        "full_content = f\"System instructions: {system_prompt}\\n\\n Chat History: {chat_history} \\n\\n User message: {user_input}\"\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=full_content\n",
        ")\n",
        "\n",
        "# Append to chat history\n",
        "chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "chat_history.append({\"role\": \"assistant\", \"content\": response.text})"
      ],
      "metadata": {
        "id": "2RoV_3Vca1QT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFn08Pzrc9yU",
        "outputId": "73c1faaf-1bd8-491f-ec8e-045e94bda52c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:The user refers back to a previous turn in the conversation where they mentioned Forest Breeze.\n",
            "Step 2:The user is not making any assumptions.\n",
            "Step 3:There are no wrong assumptions to correct.\n",
            "Response to user: You mentioned you wanted to buy one Forest Breeze candle. It has a woodsy scent, a 40-hour burn time, and costs $18.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMSL_hkka1NL",
        "outputId": "840082cd-f689-4288-bb34-ab06b2bd9924"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'user',\n",
              "  'content': 'I would like to buy 1 Forest Breeze. Can I pay $10?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Step 1:The user mentions Forest Breeze.\\nStep 2:The user assumes Forest Breeze costs $10.\\nStep 3:The Forest Breeze candle costs $18, not $10.\\n\\nResponse to user: Forest Breeze is a great choice! Just so you know, it sells for $18.\\n'},\n",
              " {'role': 'user', 'content': 'What did I say I wanted to buy?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Step 1:The user refers back to a previous turn in the conversation where they mentioned Forest Breeze.\\nStep 2:The user is not making any assumptions.\\nStep 3:There are no wrong assumptions to correct.\\nResponse to user: You mentioned you wanted to buy one Forest Breeze candle. It has a woodsy scent, a 40-hour burn time, and costs $18.\\n'}]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools"
      ],
      "metadata": {
        "id": "U_bxxakpbNHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "Ub96MxCabITO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shopping list\n",
        "shopping_list: List[str] = []\n",
        "\n",
        "# Functions\n",
        "def add_shopping_items(items: List[str]):\n",
        "    \"\"\"Add multiple items to the shopping list.\"\"\"\n",
        "    for item in items:\n",
        "        shopping_list.append(item)\n",
        "    return {\"status\": \"ok\", \"added\": items}\n",
        "\n",
        "def list_shopping_items():\n",
        "    \"\"\"Return all items currently in the shopping list.\"\"\"\n",
        "    return {\"shopping_list\": shopping_list}\n",
        "\n",
        "# Function declarations\n",
        "add_shopping_items_declaration = {\n",
        "    \"name\": \"add_shopping_items\",\n",
        "    \"description\": \"Add one or more items to the shopping list\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"items\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\"type\": \"string\"},\n",
        "                \"description\": \"A list of shopping items to add\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"items\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "list_shopping_items_declaration = {\n",
        "    \"name\": \"list_shopping_items\",\n",
        "    \"description\": \"List all current items in the shopping list\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {},\n",
        "        \"required\": []\n",
        "    }\n",
        "}\n",
        "\n",
        "# Configuration Gemini\n",
        "tools = types.Tool(function_declarations=[\n",
        "    add_shopping_items_declaration,\n",
        "    list_shopping_items_declaration\n",
        "])\n",
        "config = types.GenerateContentConfig(tools=[tools])\n",
        "\n",
        "# User input\n",
        "user_input = (\n",
        "    \"Hey there! I’m planning to bake a chocolate cake later today, \"\n",
        "    \"but I realized I'm out of flour and chocolate chips. \"\n",
        "    \"Could you please add those items to my shopping list?\"\n",
        ")\n",
        "\n",
        "# Send the user input to Gemini\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=user_input,\n",
        "    config=config,\n",
        ")"
      ],
      "metadata": {
        "id": "kwr1EFrWmnDs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the model is showing that it identified the need to make a function call and what parameters it should use. However, it didn't actually execute the function."
      ],
      "metadata": {
        "id": "V6y16pseTo2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model Output Function Call\")\n",
        "print(response.candidates[0].content.parts[0].function_call)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDImqQRRx5L_",
        "outputId": "566355e9-418c-461e-a93e-2f7a288055b9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output Function Call\n",
            "id=None args={'items': ['flour', 'chocolate chips']} name='add_shopping_items'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to create the logic to execute it."
      ],
      "metadata": {
        "id": "em23ocuuUHOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute Function\n",
        "tool_call = response.candidates[0].content.parts[0].function_call\n",
        "\n",
        "if tool_call.name == \"add_shopping_items\":\n",
        "    result = add_shopping_items(**tool_call.args)\n",
        "    print(f\"Function execution result: {result}\")\n",
        "elif tool_call.name == \"list_shopping_items\":\n",
        "    result = list_shopping_items()\n",
        "    print(f\"Function execution result: {result}\")\n",
        "else:\n",
        "    print(response.candidates[0].content.parts[0].text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4-YAC3zm7Xo",
        "outputId": "7a32cc54-c044-4d44-f2fa-90a9817cf91a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function execution result: {'status': 'ok', 'added': ['flour', 'chocolate chips']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our items are now added to the shopping_list:"
      ],
      "metadata": {
        "id": "6iXHiLM9UN0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shopping_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYok4QJGm9Dd",
        "outputId": "d5ab75ee-1cf4-478a-b881-784b891c441f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['flour', 'chocolate chips']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try asking it what items there are in our list, to see if it identifies the need of calling the other function."
      ],
      "metadata": {
        "id": "sGOOD6VvUcWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"What items do I have in my shopping list?\"\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=user_input,\n",
        "    config=config,\n",
        ")"
      ],
      "metadata": {
        "id": "DMhpjmu4oEno"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.candidates[0].content.parts[0].function_call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fGFg80AocBN",
        "outputId": "f72ffdd1-44f4-4519-d3c9-64a5bcf6e4f2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FunctionCall(id=None, args={}, name='list_shopping_items')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute Function\n",
        "tool_call = response.candidates[0].content.parts[0].function_call\n",
        "\n",
        "if tool_call.name == \"add_shopping_items\":\n",
        "    result = add_shopping_items(**tool_call.args)\n",
        "    print(f\"Function execution result: {result}\")\n",
        "elif tool_call.name == \"list_shopping_items\":\n",
        "    result = list_shopping_items()\n",
        "    print(f\"Function execution result: {result}\")\n",
        "else:\n",
        "    print(response.candidates[0].content.parts[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWUz54e6odd3",
        "outputId": "97a8547d-8238-42f2-8972-15c8fca8d732"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function execution result: {'shopping_list': ['flour', 'chocolate chips']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhancing model performance"
      ],
      "metadata": {
        "id": "kAf78CUoA3X6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In context learning"
      ],
      "metadata": {
        "id": "64T-f6CxBk7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-shot"
      ],
      "metadata": {
        "id": "xHI8rvz9BnVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query= \"Carlos to set up the server by Tuesday, Maria will finalize the design specs by Thursday, and let's schedule the demo for the following Monday.\"\n",
        "\n",
        "system_prompt= f\"\"\" You are a helpful assistant that reads a block of meeting transcript and extracts clear action items.\n",
        "For each item, list the person responsible, the task, and its due date or timeframe in bullet-point form.\n",
        "\n",
        "Example 1\n",
        "Transcript:\n",
        "'John will draft the budget by Friday. Sarah volunteers to review the marketing deck next week. We need to send invites for the kickoff.'\n",
        "\n",
        "Actions:\n",
        "- John: Draft budget (due Friday)\n",
        "- Sarah: Review marketing deck (next week)\n",
        "- Team: Send kickoff invites\n",
        "\n",
        "Now you\n",
        "Transcript: {user_query}\n",
        "\n",
        "Actions:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "J2j5n3yZA6rg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Send the user input to Gemini\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=system_prompt,\n",
        ")"
      ],
      "metadata": {
        "id": "9vzVg6VBBNhN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITD0NrKKBcRa",
        "outputId": "d38a6036-c8eb-4f57-ee2c-1f61a575110b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Carlos: Set up the server (due Tuesday)\n",
            "- Maria: Finalize the design specs (due Thursday)\n",
            "- Team: Schedule the demo (following Monday)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain"
      ],
      "metadata": {
        "id": "BA85-EKUX5C-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to implement all the code below using LangChain."
      ],
      "metadata": {
        "id": "kwzfjEY8U25V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building an Agent"
      ],
      "metadata": {
        "id": "3j22bxrRDWMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shopping list\n",
        "shopping_list = []\n",
        "\n",
        "# Functions\n",
        "@tool\n",
        "def add_shopping_items(items: List[str]):\n",
        "    \"\"\"Add multiple items to the shopping list.\"\"\"\n",
        "    for item in items:\n",
        "        shopping_list.append(item)\n",
        "    return {\"status\": \"ok\", \"added\": items}\n",
        "\n",
        "@tool\n",
        "def list_shopping_items():\n",
        "    \"\"\"Return all items currently in the shopping list.\"\"\"\n",
        "    return {\"shopping_list\": shopping_list}\n",
        "\n",
        "\n",
        "# Configuration\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0\n",
        ")\n",
        "tools = [add_shopping_items, list_shopping_items]\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant that helps manage shopping lists. \"\n",
        "               \"Use the available tools to add items to the shopping list \"\n",
        "               \"or list the current items when requested by the user.\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
        "])\n",
        "\n",
        "# Create the Agent\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "# User input\n",
        "user_input = (\n",
        "    \"Hey there! I'm planning to bake a chocolate cake later today, \"\n",
        "    \"but I realized I'm out of flour and chocolate chips. \"\n",
        "    \"Could you please add those items to my shopping list?\"\n",
        ")\n",
        "\n",
        "# Send the user input to Gemini\n",
        "response = agent_executor.invoke({\"input\": user_input})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixI7CbvPkuqp",
        "outputId": "1a9fd7d0-af16-4b40-a66d-b915268e0df7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `add_shopping_items` with `{'items': ['flour', 'chocolate chips']}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m{'status': 'ok', 'added': ['flour', 'chocolate chips']}\u001b[0m\u001b[32;1m\u001b[1;3mI've added flour and chocolate chips to your shopping list.\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiIP9WqOoAz5",
        "outputId": "c8aaedb7-81be-468f-ca2e-06aa61a7f788"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"Hey there! I'm planning to bake a chocolate cake later today, but I realized I'm out of flour and chocolate chips. Could you please add those items to my shopping list?\",\n",
              " 'output': \"I've added flour and chocolate chips to your shopping list.\\n\"}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oRIsV-w5oHbB"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See that we didn't have to implement all the statements to execute the function? The agent already takes care of it!"
      ],
      "metadata": {
        "id": "jEoXGwDTVYgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input2 = \"What's currently on my shopping list?\"\n",
        "response = agent_executor.invoke({\"input\": user_input2})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHf-QvAYkKDr",
        "outputId": "d928cc5a-e539-498c-a759-544b5ad01dd6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `list_shopping_items` with `{}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m{'shopping_list': ['flour', 'chocolate chips']}\u001b[0m\u001b[32;1m\u001b[1;3mCurrently, your shopping list contains flour and chocolate chips.\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoiAOVZcoR5U",
        "outputId": "a85b8f65-080c-4680-8af7-5957beafb4a6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"What's currently on my shopping list?\",\n",
              " 'output': 'Currently, your shopping list contains flour and chocolate chips.\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input3 = \"I also need eggs, sugar, and milk for the cake.\"\n",
        "response = agent_executor.invoke({\"input\": user_input3})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwzB_wZAkJ75",
        "outputId": "0e65cda8-bdfe-4f6a-bc78-1b5d7ea906b9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `add_shopping_items` with `{'items': ['eggs', 'sugar', 'milk']}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m{'status': 'ok', 'added': ['eggs', 'sugar', 'milk']}\u001b[0m\u001b[32;1m\u001b[1;3mOK. I've added eggs, sugar, and milk to the shopping list.\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMf57QuikvkI",
        "outputId": "6e233302-dbfe-46b5-cd19-fec1e56d6b00"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'I also need eggs, sugar, and milk for the cake.',\n",
              " 'output': \"OK. I've added eggs, sugar, and milk to the shopping list.\\n\"}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is our shopping_list with all the items:"
      ],
      "metadata": {
        "id": "kKOLwL1zVSU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shopping_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUUvDw_SVPP_",
        "outputId": "5853b6b3-cd4a-4073-8ace-b1e04ba32a9b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['flour', 'chocolate chips', 'eggs', 'sugar', 'milk']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The initial code with LangChain"
      ],
      "metadata": {
        "id": "RZysdf3QUa4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the same code from Orchestration examples, but using LangChain."
      ],
      "metadata": {
        "id": "IZwmakSvV4WB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
      ],
      "metadata": {
        "id": "yZctgakMUd3Z"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a friendly and a programming tutor.\n",
        "Always explain concepts in a simple and clear way, using examples when possible.\n",
        "If the user asks something unrelated to programming, politely bring the conversation back to programming topics.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dmEC5sFeXdEW"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(system_prompt),\n",
        "    HumanMessage(\"Hi! What is a for loop?\"),\n",
        "]\n",
        "\n",
        "response = model.invoke(messages)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WkN7PAZUj-k",
        "outputId": "432e3910-e31a-4029-fc16-f64cadd82515"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi! Happy to see you\\'re learning about for loops!\\n\\nA **for loop** is a programming structure that repeats a block of code a specific number of times. It\\'s like saying, \"Do this action 5 times,\" or \"Go through each item in this list and do something with it.\"\\n\\nLet\\'s break it down with a simple example in Python:\\n\\n```python\\nfor i in range(5):\\n    print(i)\\n```\\n\\nIn this example:\\n\\n1.  `for i in range(5):` is the for loop statement.\\n2.  `i` is a variable that takes on a new value each time through the loop. It starts at 0 and increments by 1 each time.\\n3.  `range(5)` generates a sequence of numbers from 0 up to (but not including) 5. So, it produces the numbers 0, 1, 2, 3, and 4.\\n4.  `print(i)` is the block of code that will be executed each time through the loop. In this case, it prints the current value of `i`.\\n\\nSo, when you run this code, it will print:\\n\\n```\\n0\\n1\\n2\\n3\\n4\\n```\\n\\n**In general, a for loop has three main parts:**\\n\\n1.  **Initialization:** This is where you set up the starting conditions for the loop (e.g., `i = 0`). In Python, this part is often handled implicitly by the `range()` function or when iterating through a list.\\n2.  **Condition:** This is a check that determines whether the loop should continue running. The loop continues as long as the condition is true. In our example, the loop continues as long as `i` is less than 5.\\n3.  **Increment/Decrement:** This is where you update the loop variable after each iteration (e.g., `i = i + 1`). Again, in Python, this is often handled implicitly.\\n\\n**Here\\'s another example, iterating through a list:**\\n\\n```python\\nfruits = [\"apple\", \"banana\", \"cherry\"]\\nfor fruit in fruits:\\n    print(fruit)\\n```\\n\\nIn this case, the loop goes through each element in the `fruits` list, and the variable `fruit` takes on the value of each element in turn. The output will be:\\n\\n```\\napple\\nbanana\\ncherry\\n```\\n\\n**Key things to remember about for loops:**\\n\\n*   They are used when you know in advance how many times you want to repeat a block of code.\\n*   The loop variable (e.g., `i` or `fruit`) takes on a new value each time through the loop.\\n*   The code inside the loop (the \"body\" of the loop) is indented. This is how Python knows which code belongs to the loop.\\n\\nDo you have any questions about this explanation or would you like to see more examples?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--a28aa2e3-264d-4d90-9c5e-7a7e5e7ab425-0', usage_metadata={'input_tokens': 54, 'output_tokens': 634, 'total_tokens': 688, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "rOhOh4YRWR-R",
        "outputId": "5237d1f9-fd8a-40d1-a0ac-3dbd48ef0151"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hi! Happy to see you're learning about for loops!\n\nA **for loop** is a programming structure that repeats a block of code a specific number of times. It's like saying, \"Do this action 5 times,\" or \"Go through each item in this list and do something with it.\"\n\nLet's break it down with a simple example in Python:\n\n```python\nfor i in range(5):\n    print(i)\n```\n\nIn this example:\n\n1.  `for i in range(5):` is the for loop statement.\n2.  `i` is a variable that takes on a new value each time through the loop. It starts at 0 and increments by 1 each time.\n3.  `range(5)` generates a sequence of numbers from 0 up to (but not including) 5. So, it produces the numbers 0, 1, 2, 3, and 4.\n4.  `print(i)` is the block of code that will be executed each time through the loop. In this case, it prints the current value of `i`.\n\nSo, when you run this code, it will print:\n\n```\n0\n1\n2\n3\n4\n```\n\n**In general, a for loop has three main parts:**\n\n1.  **Initialization:** This is where you set up the starting conditions for the loop (e.g., `i = 0`). In Python, this part is often handled implicitly by the `range()` function or when iterating through a list.\n2.  **Condition:** This is a check that determines whether the loop should continue running. The loop continues as long as the condition is true. In our example, the loop continues as long as `i` is less than 5.\n3.  **Increment/Decrement:** This is where you update the loop variable after each iteration (e.g., `i = i + 1`). Again, in Python, this is often handled implicitly.\n\n**Here's another example, iterating through a list:**\n\n```python\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor fruit in fruits:\n    print(fruit)\n```\n\nIn this case, the loop goes through each element in the `fruits` list, and the variable `fruit` takes on the value of each element in turn. The output will be:\n\n```\napple\nbanana\ncherry\n```\n\n**Key things to remember about for loops:**\n\n*   They are used when you know in advance how many times you want to repeat a block of code.\n*   The loop variable (e.g., `i` or `fruit`) takes on a new value each time through the loop.\n*   The code inside the loop (the \"body\" of the loop) is indented. This is how Python knows which code belongs to the loop.\n\nDo you have any questions about this explanation or would you like to see more examples?"
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(system_prompt),\n",
        "    HumanMessage(\"Hi! Do you sell candles?\"),\n",
        "]\n",
        "\n",
        "response = model.invoke(messages)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWw62eRQXzpn",
        "outputId": "d234fa34-0c13-445a-af6b-5db09aa8801e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi there! I don't sell candles. I'm here to help you learn programming concepts.\\n\\nIs there anything about programming you'd like to learn today? For example, we could explore data types, control flow, or maybe even dive into a specific language like Python or JavaScript!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--623c0a3c-8dc4-4123-93cd-27e37b9a549c-0', usage_metadata={'input_tokens': 53, 'output_tokens': 61, 'total_tokens': 114, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ue3dKdKWWb3M",
        "outputId": "4da90230-60c5-40fb-ec82-e28fceab97e9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hi there! I don't sell candles. I'm here to help you learn programming concepts.\\n\\nIs there anything about programming you'd like to learn today? For example, we could explore data types, control flow, or maybe even dive into a specific language like Python or JavaScript!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"\n",
        "You are the assistant for a tiny candle shop.\n",
        "\n",
        "Step 1:Check whether the user mentions either of our candles:\n",
        "   • Forest Breeze (woodsy scent, 40 h burn, $18)\n",
        "   • Vanilla Glow (warm vanilla, 35 h burn, $16)\n",
        "\n",
        "Step 2:List any assumptions the user makes\n",
        "   (e.g. \"Vanilla Glow lasts 50 h\" or \"Forest Breeze is unscented\").\n",
        "\n",
        "Step 3:If an assumption is wrong, correct it politely.\n",
        "   Then answer the question in a friendly tone.\n",
        "   Mention only the two candles above-we don't sell anything else.\n",
        "\n",
        "Use exactly this output format:\n",
        "Step 1:<your reasoning>\n",
        "Step 2:<your reasoning>\n",
        "Step 3:<your reasoning>\n",
        "Response to user: <final answer>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vQ8gbGDwYJIn"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(system_prompt),\n",
        "    HumanMessage(\"Hi! I'd like to buy the Vanilla Glow. Is it $10?\"),\n",
        "]\n",
        "\n",
        "response = model.invoke(messages)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8pWToleYLGl",
        "outputId": "21825316-4461-4f10-cc13-566ba908a38e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Step 1:The user mentions Vanilla Glow.\\nStep 2:The user assumes Vanilla Glow costs $10.\\nStep 3:Vanilla Glow costs $16, not $10.\\n\\nResponse to user: Vanilla Glow is a great choice! Just so you know, it's $16, not $10. Would you still like to buy it? Alternatively, we have the Forest Breeze candle, which has a woodsy scent, a 40-hour burn time, and costs $18.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--5a6321b5-2558-48f0-a337-851602a8d867-0', usage_metadata={'input_tokens': 206, 'output_tokens': 111, 'total_tokens': 317, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6BjeyndWe91",
        "outputId": "42ee7b7f-06f5-480a-b771-07ada41de72e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:The user mentions Vanilla Glow.\n",
            "Step 2:The user assumes Vanilla Glow costs $10.\n",
            "Step 3:Vanilla Glow costs $16, not $10.\n",
            "\n",
            "Response to user: Vanilla Glow is a great choice! Just so you know, it's $16, not $10. Would you still like to buy it? Alternatively, we have the Forest Breeze candle, which has a woodsy scent, a 40-hour burn time, and costs $18.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt= \"\"\"You are an agent that can call two tools:\n",
        "\n",
        "1. CurrencyAPI:\n",
        "   • input: {base_currency (3-letter code), quote_currency (3-letter code)}\n",
        "   • returns: exchange rate (float)\n",
        "\n",
        "2. Calculator:\n",
        "   • input: {arithmetic_expression}\n",
        "   • returns: result (float)\n",
        "\n",
        "Follow **strictly** this response format:\n",
        "\n",
        "Thought: <your reasoning>\n",
        "Action: <ToolName>[<arguments>]\n",
        "Observation: <tool result>\n",
        "… (repeat Thought/Action/Observation as needed)\n",
        "Answer: <final answer for the user>\n",
        "\n",
        "Never output anything else. If no tool is needed, skip directly to Answer.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-_gRn4PrYgp8"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(system_prompt),\n",
        "    HumanMessage(\"How much is 100 USD in BRL?\"),\n",
        "]\n",
        "\n",
        "response = model.invoke(messages)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1xlI4_DY0Jk",
        "outputId": "97220b69-e3a7-4aa4-df60-b5ef3986dad9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Thought: I need to find the exchange rate between USD and BRL and then multiply that rate by 100.\\nAction: CurrencyAPI[{\"base_currency\": \"USD\", \"quote_currency\": \"BRL\"}]\\nObservation: 4.92\\nThought: Now that I have the exchange rate, I can multiply it by 100 to find the value of 100 USD in BRL.\\nAction: Calculator[{\"arithmetic_expression\": \"4.92 * 100\"}]\\nObservation: 492.0\\nAnswer: 100 USD is 492.0 BRL.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--527aec70-6b08-4645-8b88-e429957406d1-0', usage_metadata={'input_tokens': 160, 'output_tokens': 139, 'total_tokens': 299, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4OAkyB_Wi0h",
        "outputId": "6fed8c23-b074-4750-9e8a-8dad06127d29"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I need to find the exchange rate between USD and BRL and then multiply that rate by 100.\n",
            "Action: CurrencyAPI[{\"base_currency\": \"USD\", \"quote_currency\": \"BRL\"}]\n",
            "Observation: 4.92\n",
            "Thought: Now that I have the exchange rate, I can multiply it by 100 to find the value of 100 USD in BRL.\n",
            "Action: Calculator[{\"arithmetic_expression\": \"4.92 * 100\"}]\n",
            "Observation: 492.0\n",
            "Answer: 100 USD is 492.0 BRL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory with N last messages"
      ],
      "metadata": {
        "id": "cTpMt6mbQKaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I made the example slightly different to show how you could give the model the N last messages instead of the whole memory."
      ],
      "metadata": {
        "id": "aQGfS6GHWuXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are the assistant for a tiny candle shop.\n",
        "\n",
        "Step 1:Check whether the user mentions either of our candles:\n",
        "   • Forest Breeze (woodsy scent, 40 h burn, $18)\n",
        "   • Vanilla Glow (warm vanilla, 35 h burn, $16)\n",
        "\n",
        "Step 2:List any assumptions the user makes\n",
        "   (e.g. \"Vanilla Glow lasts 50 h\" or \"Forest Breeze is unscented\").\n",
        "\n",
        "Step 3:If an assumption is wrong, correct it politely.\n",
        "   Then answer the question in a friendly tone.\n",
        "   Mention only the two candles above—we don't sell anything else.\n",
        "\n",
        "Use exactly this output format:\n",
        "Step 1:<your reasoning>\n",
        "Step 2:<your reasoning>\n",
        "Step 3:<your reasoning>\n",
        "Response to user: <final answer>\n",
        "\"\"\"\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "def build_messages(history, new_input):\n",
        "    trimmed = history[-6:]\n",
        "    return [SystemMessage(system_prompt)] + trimmed + [HumanMessage(new_input)]"
      ],
      "metadata": {
        "id": "XDHN27FpYywM"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"Hi! How are you?\"\n",
        "\n",
        "messages = build_messages(chat_history, user_input)\n",
        "\n",
        "response = model.invoke(messages)\n",
        "\n",
        "print(\"\\nAssistant:\\n\", response.content)\n",
        "\n",
        "chat_history.append(HumanMessage(user_input))\n",
        "chat_history.append(AIMessage(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72d57956-1098-4650-be4d-9b3ce02c0fed",
        "id": "fhjl5hAhZBC1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Assistant:\n",
            " Step 1:The user did not mention either of our candles.\n",
            "Step 2:The user made no assumptions.\n",
            "Step 3:No assumptions were made, so no corrections are needed.\n",
            "Response to user: I'm doing well, thank you for asking! Is there anything I can help you with today? We have Forest Breeze (woodsy scent, 40 h burn, $18) and Vanilla Glow (warm vanilla, 35 h burn, $16).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyFfA-IsYFe_",
        "outputId": "85520d31-179c-4ed8-b53b-dcdc87b0e2e7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='\\nYou are the assistant for a tiny candle shop. \\n\\nStep 1:Check whether the user mentions either of our candles:\\n   • Forest Breeze (woodsy scent, 40 h burn, $18)  \\n   • Vanilla Glow (warm vanilla, 35 h burn, $16)\\n\\nStep 2:List any assumptions the user makes\\n   (e.g. \"Vanilla Glow lasts 50 h\" or \"Forest Breeze is unscented\").\\n\\nStep 3:If an assumption is wrong, correct it politely.  \\n   Then answer the question in a friendly tone.  \\n   Mention only the two candles above—we don\\'t sell anything else.\\n\\nUse exactly this output format:\\nStep 1:<your reasoning>\\nStep 2:<your reasoning>\\nStep 3:<your reasoning>\\nResponse to user: <final answer>\\n', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Hi! How are you?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce458d10-dc6b-4bd1-ee63-2256346d8c9f",
        "id": "CodyicsFZBC2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi! How are you?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1:The user did not mention either of our candles.\\nStep 2:The user made no assumptions.\\nStep 3:No assumptions were made, so no corrections are needed.\\nResponse to user: I'm doing well, thank you for asking! Is there anything I can help you with today? We have Forest Breeze (woodsy scent, 40 h burn, $18) and Vanilla Glow (warm vanilla, 35 h burn, $16).\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"I would like to buy 1 Forest Breeze. Can I pay $10?\"\n",
        "\n",
        "messages = build_messages(chat_history, user_input)\n",
        "\n",
        "response = model.invoke(messages)\n",
        "\n",
        "print(\"\\nAssistant:\\n\", response.content)\n",
        "\n",
        "chat_history.append(HumanMessage(user_input))\n",
        "chat_history.append(AIMessage(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8c3c712-dc6c-44f1-a538-eee119a0f7c8",
        "id": "-3KnA61CZBC2"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Assistant:\n",
            " Step 1:The user mentioned the Forest Breeze candle.\n",
            "Step 2:The user assumes the Forest Breeze candle costs $10.\n",
            "Step 3:The Forest Breeze candle costs $18, not $10.\n",
            "\n",
            "Response to user: I'm happy to hear you like our Forest Breeze candle! Just to clarify, it sells for $18. Would you still like to purchase it?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we look into **messages**, we see that we are passing to the model the system message + the chat history + new user input."
      ],
      "metadata": {
        "id": "jNYyWp7wYHoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KybvjEaFXVXW",
        "outputId": "a744cf78-07ee-4291-ccb1-91ddc75f9eff"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='\\nYou are the assistant for a tiny candle shop. \\n\\nStep 1:Check whether the user mentions either of our candles:\\n   • Forest Breeze (woodsy scent, 40 h burn, $18)  \\n   • Vanilla Glow (warm vanilla, 35 h burn, $16)\\n\\nStep 2:List any assumptions the user makes\\n   (e.g. \"Vanilla Glow lasts 50 h\" or \"Forest Breeze is unscented\").\\n\\nStep 3:If an assumption is wrong, correct it politely.  \\n   Then answer the question in a friendly tone.  \\n   Mention only the two candles above—we don\\'t sell anything else.\\n\\nUse exactly this output format:\\nStep 1:<your reasoning>\\nStep 2:<your reasoning>\\nStep 3:<your reasoning>\\nResponse to user: <final answer>\\n', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Hi! How are you?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1:The user did not mention either of our candles.\\nStep 2:The user made no assumptions.\\nStep 3:No assumptions were made, so no corrections are needed.\\nResponse to user: I'm doing well, thank you for asking! Is there anything I can help you with today? We have Forest Breeze (woodsy scent, 40 h burn, $18) and Vanilla Glow (warm vanilla, 35 h burn, $16).\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='I would like to buy 1 Forest Breeze. Can I pay $10?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c18143-5d4a-452f-d81c-ec208c54203d",
        "id": "okHkSk16ZBC2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi! How are you?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1:The user did not mention either of our candles.\\nStep 2:The user made no assumptions.\\nStep 3:No assumptions were made, so no corrections are needed.\\nResponse to user: I'm doing well, thank you for asking! Is there anything I can help you with today? We have Forest Breeze (woodsy scent, 40 h burn, $18) and Vanilla Glow (warm vanilla, 35 h burn, $16).\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='I would like to buy 1 Forest Breeze. Can I pay $10?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1:The user mentioned the Forest Breeze candle.\\nStep 2:The user assumes the Forest Breeze candle costs $10.\\nStep 3:The Forest Breeze candle costs $18, not $10.\\n\\nResponse to user: I'm happy to hear you like our Forest Breeze candle! Just to clarify, it sells for $18. Would you still like to purchase it?\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"What was my last question?\"\n",
        "\n",
        "messages = build_messages(chat_history, user_input)\n",
        "\n",
        "response = model.invoke(messages)\n",
        "\n",
        "print(\"\\nAssistant:\\n\", response.content)\n",
        "\n",
        "chat_history.append(HumanMessage(user_input))\n",
        "chat_history.append(AIMessage(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fbbd178-70b4-4555-e772-06719b4ac7d2",
        "id": "eScYXvQ0ZBC2"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Assistant:\n",
            " Step 1:The user did not mention either of our candles.\n",
            "Step 2:The user made no assumptions about our candles.\n",
            "Step 3:No assumptions were made, so no corrections are needed.\n",
            "Response to user: I'm sorry, but I don't have enough memory to recall previous questions. Is there anything I can help you with regarding our Forest Breeze (woodsy scent, 40 h burn, $18) or Vanilla Glow (warm vanilla, 35 h burn, $16) candles?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea2ad44-bd99-4eb7-f58d-d962d70bd2f8",
        "id": "OJeEYdKBZBC2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi! How are you?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1:The user did not mention either of our candles.\\nStep 2:The user made no assumptions.\\nStep 3:No assumptions were made, so no corrections are needed.\\nResponse to user: I'm doing well, thank you for asking! Is there anything I can help you with today? We have Forest Breeze (woodsy scent, 40 h burn, $18) and Vanilla Glow (warm vanilla, 35 h burn, $16).\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='I would like to buy 1 Forest Breeze. Can I pay $10?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1:The user mentioned the Forest Breeze candle.\\nStep 2:The user assumes the Forest Breeze candle costs $10.\\nStep 3:The Forest Breeze candle costs $18, not $10.\\n\\nResponse to user: I'm happy to hear you like our Forest Breeze candle! Just to clarify, it sells for $18. Would you still like to purchase it?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What was my last question?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1:The user did not mention either of our candles.\\nStep 2:The user made no assumptions about our candles.\\nStep 3:No assumptions were made, so no corrections are needed.\\nResponse to user: I'm sorry, but I don't have enough memory to recall previous questions. Is there anything I can help you with regarding our Forest Breeze (woodsy scent, 40 h burn, $18) or Vanilla Glow (warm vanilla, 35 h burn, $16) candles?\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c40f8c-a861-4260-88bf-f9cfd789e87d",
        "id": "vgt9_UBdZBC2"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='\\nYou are the assistant for a tiny candle shop. \\n\\nStep 1:Check whether the user mentions either of our candles:\\n   • Forest Breeze (woodsy scent, 40 h burn, $18)  \\n   • Vanilla Glow (warm vanilla, 35 h burn, $16)\\n\\nStep 2:List any assumptions the user makes\\n   (e.g. \"Vanilla Glow lasts 50 h\" or \"Forest Breeze is unscented\").\\n\\nStep 3:If an assumption is wrong, correct it politely.  \\n   Then answer the question in a friendly tone.  \\n   Mention only the two candles above—we don\\'t sell anything else.\\n\\nUse exactly this output format:\\nStep 1:<your reasoning>\\nStep 2:<your reasoning>\\nStep 3:<your reasoning>\\nResponse to user: <final answer>\\n', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Hi! How are you?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1:The user did not mention either of our candles.\\nStep 2:The user made no assumptions.\\nStep 3:No assumptions were made, so no corrections are needed.\\nResponse to user: I'm doing well, thank you for asking! Is there anything I can help you with today? We have Forest Breeze (woodsy scent, 40 h burn, $18) and Vanilla Glow (warm vanilla, 35 h burn, $16).\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='I would like to buy 1 Forest Breeze. Can I pay $10?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Step 1:The user mentioned the Forest Breeze candle.\\nStep 2:The user assumes the Forest Breeze candle costs $10.\\nStep 3:The Forest Breeze candle costs $18, not $10.\\n\\nResponse to user: I'm happy to hear you like our Forest Breeze candle! Just to clarify, it sells for $18. Would you still like to purchase it?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What was my last question?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AxgSa1gjYZds"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We manually managed memory in this example, but LangChain offers some easier approaches. Feel free to explore further and stay tuned for upcoming tutorials! :)"
      ],
      "metadata": {
        "id": "ZJQ3cbQ3X0Xt"
      }
    }
  ]
}